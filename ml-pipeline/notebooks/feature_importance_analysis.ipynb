{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90849aa5",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SYMBOLS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA']\n",
    "LOOKBACK_DAYS = 252  # 1 year of trading days\n",
    "PREDICTION_HORIZON = 5  # Predict 5-day returns\n",
    "\n",
    "# Feature categories\n",
    "TECHNICAL_FEATURES = [\n",
    "    # Momentum\n",
    "    'rsi_14', 'rsi_7', 'stochastic_k', 'stochastic_d', 'williams_r',\n",
    "    'roc_10', 'roc_20', 'momentum_10',\n",
    "    # Trend\n",
    "    'macd_line', 'macd_signal', 'macd_histogram', 'adx_14',\n",
    "    'plus_di', 'minus_di', 'aroon_oscillator', 'cci_20',\n",
    "    # Moving Averages\n",
    "    'price_vs_sma_20', 'price_vs_sma_50', 'price_vs_sma_200',\n",
    "    # Volatility\n",
    "    'bb_width', 'bb_percent_b', 'atr_percent', 'volatility_20',\n",
    "    # Volume\n",
    "    'mfi_14', 'volume_ratio'\n",
    "]\n",
    "\n",
    "FUNDAMENTAL_FEATURES = [\n",
    "    'pe_ratio', 'forward_pe', 'peg_ratio', 'pb_ratio', 'ps_ratio',\n",
    "    'roe', 'roa', 'roic', 'gross_margin', 'operating_margin',\n",
    "    'net_margin', 'debt_to_equity', 'current_ratio', 'quick_ratio',\n",
    "    'revenue_growth', 'earnings_growth', 'dividend_yield'\n",
    "]\n",
    "\n",
    "MARKET_FEATURES = [\n",
    "    'spy_correlation_20', 'spy_correlation_60', 'spy_beta',\n",
    "    'sector_relative_strength', 'market_regime',\n",
    "    'vix_level', 'put_call_ratio'\n",
    "]\n",
    "\n",
    "ALL_FEATURES = TECHNICAL_FEATURES + FUNDAMENTAL_FEATURES + MARKET_FEATURES\n",
    "print(f\"Total features: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_features(n_samples=1000, n_features=50):\n",
    "    \"\"\"\n",
    "    Generate synthetic feature data for demonstration.\n",
    "    In production, this would load from the FeatureStore.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate base features\n",
    "    X = pd.DataFrame()\n",
    "    \n",
    "    # Technical features (some correlated with target)\n",
    "    X['rsi_14'] = np.random.uniform(20, 80, n_samples)\n",
    "    X['rsi_7'] = X['rsi_14'] + np.random.normal(0, 5, n_samples)\n",
    "    X['macd_histogram'] = np.random.normal(0, 1, n_samples)\n",
    "    X['adx_14'] = np.random.uniform(10, 50, n_samples)\n",
    "    X['bb_percent_b'] = np.random.uniform(0, 1, n_samples)\n",
    "    X['atr_percent'] = np.random.uniform(1, 5, n_samples)\n",
    "    X['volume_ratio'] = np.random.lognormal(0, 0.5, n_samples)\n",
    "    X['mfi_14'] = np.random.uniform(20, 80, n_samples)\n",
    "    X['price_vs_sma_20'] = np.random.normal(0, 3, n_samples)\n",
    "    X['price_vs_sma_50'] = np.random.normal(0, 5, n_samples)\n",
    "    X['momentum_10'] = np.random.normal(0, 2, n_samples)\n",
    "    X['stochastic_k'] = np.random.uniform(0, 100, n_samples)\n",
    "    \n",
    "    # Fundamental features\n",
    "    X['pe_ratio'] = np.random.lognormal(3, 0.5, n_samples)\n",
    "    X['pb_ratio'] = np.random.lognormal(1, 0.5, n_samples)\n",
    "    X['roe'] = np.random.uniform(-10, 40, n_samples)\n",
    "    X['debt_to_equity'] = np.random.lognormal(0, 0.8, n_samples)\n",
    "    X['revenue_growth'] = np.random.normal(10, 20, n_samples)\n",
    "    X['earnings_growth'] = np.random.normal(15, 30, n_samples)\n",
    "    \n",
    "    # Market features\n",
    "    X['spy_correlation_20'] = np.random.uniform(0.3, 0.95, n_samples)\n",
    "    X['spy_beta'] = np.random.uniform(0.5, 2.0, n_samples)\n",
    "    X['sector_relative_strength'] = np.random.normal(0, 1, n_samples)\n",
    "    X['vix_level'] = np.random.lognormal(2.8, 0.3, n_samples)\n",
    "    \n",
    "    # Generate target (price direction) with some feature dependency\n",
    "    prob = 0.5 + 0.01 * (X['macd_histogram'] + X['momentum_10']) \\\n",
    "              + 0.005 * (X['rsi_14'] - 50) \\\n",
    "              + 0.002 * X['sector_relative_strength']\n",
    "    prob = np.clip(prob, 0.1, 0.9)\n",
    "    y = (np.random.random(n_samples) < prob).astype(int)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_synthetic_features(n_samples=2000)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(y)}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6fe3e",
   "metadata": {},
   "source": [
    "## 2. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "feature_stats = X.describe().T\n",
    "feature_stats['missing'] = X.isnull().sum()\n",
    "feature_stats['skewness'] = X.skew()\n",
    "feature_stats['kurtosis'] = X.kurtosis()\n",
    "feature_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature distributions\n",
    "fig, axes = plt.subplots(5, 4, figsize=(16, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(X.columns):\n",
    "    if i >= len(axes):\n",
    "        break\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot distribution by target class\n",
    "    for label in [0, 1]:\n",
    "        X[y == label][col].hist(ax=ax, alpha=0.5, bins=30, \n",
    "                                 label=f'Down' if label == 0 else 'Up')\n",
    "    ax.set_title(col)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions by Target Class', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793710c3",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f',\n",
    "            cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated feature pairs\n",
    "high_corr_threshold = 0.7\n",
    "\n",
    "high_correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > high_corr_threshold:\n",
    "            high_correlations.append({\n",
    "                'feature_1': correlation_matrix.columns[i],\n",
    "                'feature_2': correlation_matrix.columns[j],\n",
    "                'correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_correlations).sort_values('correlation', \n",
    "                                                            key=abs, \n",
    "                                                            ascending=False)\n",
    "print(f\"Highly correlated feature pairs (|r| > {high_corr_threshold}):\")\n",
    "high_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "X_with_target = X.copy()\n",
    "X_with_target['target'] = y\n",
    "\n",
    "target_correlations = X_with_target.corr()['target'].drop('target').sort_values(key=abs, ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in target_correlations.values]\n",
    "plt.barh(range(len(target_correlations)), target_correlations.values, color=colors)\n",
    "plt.yticks(range(len(target_correlations)), target_correlations.index)\n",
    "plt.xlabel('Correlation with Target')\n",
    "plt.title('Feature Correlation with Target (Price Direction)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 features by absolute correlation with target:\")\n",
    "target_correlations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc893b5e",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b715cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Random Forest Feature Importance\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_importance = pd.Series(\n",
    "    rf_model.feature_importances_,\n",
    "    index=X.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_model.score(X_test, y_test))\n",
    "print(\"\\nTop 10 features by RF importance:\")\n",
    "rf_importance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Gradient Boosting Feature Importance\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "gb_importance = pd.Series(\n",
    "    gb_model.feature_importances_,\n",
    "    index=X.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"Gradient Boosting Accuracy:\", gb_model.score(X_test, y_test))\n",
    "print(\"\\nTop 10 features by GB importance:\")\n",
    "gb_importance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Mutual Information\n",
    "mi_scores = mutual_info_classif(X_scaled, y, random_state=42)\n",
    "mi_importance = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features by Mutual Information:\")\n",
    "mi_importance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b30b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: ANOVA F-statistic\n",
    "f_scores, p_values = f_classif(X_scaled, y)\n",
    "f_importance = pd.Series(f_scores, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features by F-statistic:\")\n",
    "f_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b8025",
   "metadata": {},
   "source": [
    "## 5. Combined Feature Importance Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c260cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize importance scores to 0-1 range\n",
    "def normalize_series(s):\n",
    "    return (s - s.min()) / (s.max() - s.min())\n",
    "\n",
    "# Combine all importance methods\n",
    "importance_df = pd.DataFrame({\n",
    "    'random_forest': normalize_series(rf_importance),\n",
    "    'gradient_boosting': normalize_series(gb_importance),\n",
    "    'mutual_info': normalize_series(mi_importance),\n",
    "    'f_statistic': normalize_series(f_importance),\n",
    "    'target_correlation': normalize_series(target_correlations.abs())\n",
    "})\n",
    "\n",
    "# Calculate weighted average (tree-based methods weighted higher)\n",
    "weights = {\n",
    "    'random_forest': 0.25,\n",
    "    'gradient_boosting': 0.25,\n",
    "    'mutual_info': 0.20,\n",
    "    'f_statistic': 0.15,\n",
    "    'target_correlation': 0.15\n",
    "}\n",
    "\n",
    "importance_df['combined_score'] = sum(\n",
    "    importance_df[col] * weight \n",
    "    for col, weight in weights.items()\n",
    ")\n",
    "\n",
    "importance_df = importance_df.sort_values('combined_score', ascending=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a431e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize combined feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Combined score bar chart\n",
    "ax1 = axes[0]\n",
    "top_features = importance_df.head(15)\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(top_features)))\n",
    "bars = ax1.barh(range(len(top_features)), \n",
    "                top_features['combined_score'].values[::-1],\n",
    "                color=colors[::-1])\n",
    "ax1.set_yticks(range(len(top_features)))\n",
    "ax1.set_yticklabels(top_features.index[::-1])\n",
    "ax1.set_xlabel('Combined Importance Score')\n",
    "ax1.set_title('Top 15 Features by Combined Importance')\n",
    "\n",
    "# Heatmap of all methods\n",
    "ax2 = axes[1]\n",
    "heatmap_data = importance_df.drop('combined_score', axis=1).head(15)\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            ax=ax2, cbar_kws={'label': 'Normalized Score'})\n",
    "ax2.set_title('Feature Importance by Method (Top 15)')\n",
    "ax2.set_xlabel('Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95cd43",
   "metadata": {},
   "source": [
    "## 6. Feature Selection Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features based on combined score\n",
    "TOP_N_FEATURES = 10\n",
    "\n",
    "selected_features = importance_df.head(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "print(f\"Recommended Top {TOP_N_FEATURES} Features for ML Models:\")\n",
    "print(\"=\" * 50)\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    score = importance_df.loc[feature, 'combined_score']\n",
    "    print(f\"{i:2d}. {feature:25s} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9bf3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features by type\n",
    "def categorize_feature(feature):\n",
    "    technical = ['rsi', 'macd', 'bb_', 'atr', 'adx', 'momentum', 'volume', \n",
    "                 'mfi', 'stochastic', 'price_vs_sma', 'cci', 'obv', 'roc']\n",
    "    fundamental = ['pe_', 'pb_', 'ps_', 'roe', 'roa', 'margin', 'debt', \n",
    "                   'growth', 'dividend', 'ratio', 'roic']\n",
    "    market = ['spy', 'beta', 'correlation', 'sector', 'vix', 'regime']\n",
    "    \n",
    "    feature_lower = feature.lower()\n",
    "    for t in technical:\n",
    "        if t in feature_lower:\n",
    "            return 'Technical'\n",
    "    for f in fundamental:\n",
    "        if f in feature_lower:\n",
    "            return 'Fundamental'\n",
    "    for m in market:\n",
    "        if m in feature_lower:\n",
    "            return 'Market'\n",
    "    return 'Other'\n",
    "\n",
    "# Feature category distribution\n",
    "importance_df['category'] = importance_df.index.map(categorize_feature)\n",
    "\n",
    "category_summary = importance_df.groupby('category').agg({\n",
    "    'combined_score': ['mean', 'count'],\n",
    "    'random_forest': 'mean',\n",
    "    'gradient_boosting': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nFeature Importance by Category:\")\n",
    "category_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3be27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution pie chart\n",
    "category_counts = importance_df.head(10)['category'].value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = {'Technical': '#2ecc71', 'Fundamental': '#3498db', 'Market': '#e74c3c', 'Other': '#95a5a6'}\n",
    "pie_colors = [colors.get(cat, '#95a5a6') for cat in category_counts.index]\n",
    "axes[0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "            colors=pie_colors, startangle=90)\n",
    "axes[0].set_title('Top 10 Features by Category')\n",
    "\n",
    "# Box plot of importance by category\n",
    "importance_df.boxplot(column='combined_score', by='category', ax=axes[1])\n",
    "axes[1].set_title('Importance Score Distribution by Category')\n",
    "axes[1].set_xlabel('Category')\n",
    "axes[1].set_ylabel('Combined Score')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2445b2a",
   "metadata": {},
   "source": [
    "## 7. Feature Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature importance stability with different random seeds\n",
    "n_iterations = 5\n",
    "stability_results = []\n",
    "\n",
    "for seed in range(n_iterations):\n",
    "    # Resample data\n",
    "    X_sample, _, y_sample, _ = train_test_split(X_scaled, y, test_size=0.3, random_state=seed)\n",
    "    \n",
    "    # Train RF\n",
    "    rf = RandomForestClassifier(n_estimators=50, max_depth=8, random_state=seed)\n",
    "    rf.fit(X_sample, y_sample)\n",
    "    \n",
    "    importance = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "    stability_results.append(importance)\n",
    "\n",
    "stability_df = pd.DataFrame(stability_results)\n",
    "stability_stats = pd.DataFrame({\n",
    "    'mean_importance': stability_df.mean(),\n",
    "    'std_importance': stability_df.std(),\n",
    "    'cv': stability_df.std() / stability_df.mean()  # Coefficient of variation\n",
    "}).sort_values('mean_importance', ascending=False)\n",
    "\n",
    "print(\"Feature Stability Analysis (lower CV = more stable):\")\n",
    "stability_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stability\n",
    "top_stable = stability_stats.head(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = range(len(top_stable))\n",
    "ax.bar(x, top_stable['mean_importance'], yerr=top_stable['std_importance'],\n",
    "       capsize=5, color='steelblue', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(top_stable.index, rotation=45, ha='right')\n",
    "ax.set_ylabel('Feature Importance')\n",
    "ax.set_title('Top 10 Features with Stability (Error Bars = Std Dev)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624ad4f",
   "metadata": {},
   "source": [
    "## 8. Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final feature recommendations\n",
    "final_recommendations = pd.DataFrame({\n",
    "    'combined_importance': importance_df['combined_score'],\n",
    "    'stability_cv': stability_stats['cv'],\n",
    "    'category': importance_df['category']\n",
    "})\n",
    "\n",
    "# Calculate final score (importance weighted by stability)\n",
    "final_recommendations['final_score'] = (\n",
    "    final_recommendations['combined_importance'] * \n",
    "    (1 - final_recommendations['stability_cv'].clip(0, 1))  # Penalize unstable features\n",
    ")\n",
    "\n",
    "final_recommendations = final_recommendations.sort_values('final_score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL FEATURE RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTop 10 Features (considering importance + stability):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, (feature, row) in enumerate(final_recommendations.head(10).iterrows(), 1):\n",
    "    print(f\"{i:2d}. {feature:25s} [{row['category']:12s}] Score: {row['final_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATIONS BY USE CASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüéØ For Price Direction Prediction:\")\n",
    "direction_features = final_recommendations[final_recommendations['category'].isin(['Technical', 'Market'])].head(5)\n",
    "for f in direction_features.index:\n",
    "    print(f\"   ‚Ä¢ {f}\")\n",
    "\n",
    "print(\"\\nüìä For Value Assessment:\")\n",
    "value_features = final_recommendations[final_recommendations['category'] == 'Fundamental'].head(5)\n",
    "for f in value_features.index:\n",
    "    print(f\"   ‚Ä¢ {f}\")\n",
    "\n",
    "print(\"\\nüåê For Market Context:\")\n",
    "market_features = final_recommendations[final_recommendations['category'] == 'Market'].head(3)\n",
    "for f in market_features.index:\n",
    "    print(f\"   ‚Ä¢ {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b83d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export recommendations\n",
    "export_data = {\n",
    "    'selected_features': final_recommendations.head(15).index.tolist(),\n",
    "    'feature_scores': final_recommendations.head(15)['final_score'].to_dict(),\n",
    "    'technical_features': final_recommendations[final_recommendations['category'] == 'Technical'].head(8).index.tolist(),\n",
    "    'fundamental_features': final_recommendations[final_recommendations['category'] == 'Fundamental'].head(5).index.tolist(),\n",
    "    'market_features': final_recommendations[final_recommendations['category'] == 'Market'].head(3).index.tolist()\n",
    "}\n",
    "\n",
    "print(\"\\nüìÅ Feature Selection Summary:\")\n",
    "print(f\"   Total features analyzed: {len(X.columns)}\")\n",
    "print(f\"   Selected features: {len(export_data['selected_features'])}\")\n",
    "print(f\"   - Technical: {len(export_data['technical_features'])}\")\n",
    "print(f\"   - Fundamental: {len(export_data['fundamental_features'])}\")\n",
    "print(f\"   - Market: {len(export_data['market_features'])}\")\n",
    "\n",
    "# Save to JSON for use in ML pipeline\n",
    "import json\n",
    "with open('../src/selected_features.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "print(\"\\n‚úÖ Feature selection saved to selected_features.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1ceb3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. **Most Important Technical Features**: Momentum indicators (MACD, RSI) and volatility measures (ATR, BB)\n",
    "2. **Most Important Fundamental Features**: Valuation ratios (P/E, P/B) and profitability (ROE)\n",
    "3. **Most Important Market Features**: Market correlation and sector relative strength\n",
    "\n",
    "### Recommendations:\n",
    "- Use top 10-15 features to avoid overfitting\n",
    "- Include mix of technical, fundamental, and market features\n",
    "- Monitor feature stability over time\n",
    "- Re-run analysis periodically to capture regime changes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
